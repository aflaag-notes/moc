\documentclass[a4paper, 12pt]{report}

\usepackage[dvipsnames]{xcolor}

%%%%%%%%%%%%%%%%%
% Set Variables %
%%%%%%%%%%%%%%%%%

\def\useItalian{0}  % 1 = Italian, 0 = English

\def\courseName{Models of Computation}

\def\coursePrerequisites{
    \begin{itemize}
        \item Linguaggi di Programmazione
        \item Tecniche di Programmazione Funzionale ed Imperativa
    \end{itemize}
}

\def\book{TODO}

% \def\authorName{Simone Bianco}
% \def\email{bianco.simone@outlook.it}
% \def\github{https://github.com/Exyss/university-notes}
% \def\linkedin{https://www.linkedin.com/in/simone-bianco}

\def\authorName{Alessio Bandiera}
\def\email{alessio.bandiera02@gmail.com}
\def\github{https://github.com/aflaag-notes}
\def\linkedin{https://www.linkedin.com/in/alessio-bandiera-a53767223}

% Do not change

%%%%%%%%%%%%
% Packages %
%%%%%%%%%%%%

\usepackage{../../packages/Nyx/nyx-packages}
\usepackage{../../packages/Nyx/nyx-styles}
\usepackage{../../packages/Nyx/nyx-frames}
\usepackage{../../packages/Nyx/nyx-macros}
\usepackage{../../packages/Nyx/nyx-title}
\usepackage{../../packages/Nyx/nyx-intro}

%%%%%%%%%%%%%%
% Title-page %
%%%%%%%%%%%%%%

\logo{../../packages/Nyx/logo.png}

\ifx\useItalian0
    \institute{\curlyquotes{\hspace{0.25mm}Sapienza} UniversitÃ  di Roma}
    \faculty{Ingegneria dell'Informazione,\\Informatica e Statistica}
    \department{Dipartimento di Informatica}
    \subtitle{Appunti integrati con il libro \book}
    \author{\textit{Autore}\\\authorName}
\else
    \institute{\curlyquotes{\hspace{0.25mm}Sapienza} University of Rome}
    \faculty{Faculty of Information Engineering,\\Informatics and Statistics}
    \department{Department of Computer Science}
    \subtitle{Lecture notes integrated with the book \book}
    \author{\textit{Author}\\\authorName}
\fi

\title{\courseName}
\date{\today}

% \supervisor{Linus \textsc{Torvalds}}
% \context{Well, I was bored\ldots}

%%%%%%%%%%%%
% Document %
%%%%%%%%%%%%

\begin{document}
    \maketitle

    % The following style changes are valid only inside this scope 
    {
        \hypersetup{allcolors=black}
        \fancypagestyle{plain}{%
        \fancyhead{}        % clear all header fields
        \fancyfoot{}        % clear all header fields
        \fancyfoot[C]{\thepage}
        \renewcommand{\headrulewidth}{0pt}
        \renewcommand{\footrulewidth}{0pt}}

        \romantableofcontents
    }

    \introduction

    %%%%%%%%%%%%%%%%%%%%%

    \chapter{TODO}

    \section{TODO}

    \subsection{TODO}
 
    In this first section, examples will be omitted from this notes, refer to the notes of the \curlyquotes{\href{https://github.com/aflaag-notes/languages}{Linguaggi di Programmazione}} course for further details. \todo{this notes are WIP, sections WILL change}

    \begin{frameddefn}[breakable]{Lambda calculus}
        Let $\mathrm{Var}$ be the set of all possible variables; thus, the \tbf{set $\Lambda$ of all possible $\lambda$-terms} is defined by the following rules:
        \begin{gather*}
            [var] \ \dfrac{x \in \mathrm{Var}}{x \in \Lambda} \\ \\
            [appl] \ \dfrac{M \in \Lambda \quad N \in \Lambda}{M N \in \Lambda} \\ \\
            [abs] \ \dfrac{x \in \mathrm{Var} \quad M \in \Lambda}{\lambda x.M \in \Lambda}
        \end{gather*}

        The terms of the form $\lambda x.M$ are called \tbf{$\lambda$-abstractions}, and $MN$ is the function application of $M$ to $N$. Note that function application \tit{associates to the left}, therefore $$MNL = (MN)L \neq M(NL)$$

        Lambda calculus can be alternatively defined with the \href{https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form}{Backus Normal Form} (BNF), as follows: $$M, N ::= x \mid \lambda x.M \mid M N$$
    \end{frameddefn}

    Although \tit{all functions in lambda calculus are unary}, the following definition can expand this concept.

    \begin{frameddefn}{Currying}
        \tbf{Currying} (named after \href{https://en.wikipedia.org/wiki/Haskell_Curry}{Haskell Curry}) is defined as follows: $$\lambda x_1.(\ldots (\lambda x_n. y)) \equiv \lambda x_1 \ldots x_n . y$$
    \end{frameddefn}

    Additionally, the following notation $$\lambda \vec x . f \ \vec x$$ will denote \tbf{vectors} in place of $$\lambda x_1 \ldots x_n . f \ x_1 \ldots x_n$$

    \begin{frameddefn}{Boundness}
        A variable is said to be \tbf{bound} if it is declared in a $\lambda$-abstraction, otherwise it is said to be \tbf{free}.

        A term that has no free variables is said to be \tbf{closed} or \tbf{combinator}.
    \end{frameddefn}

    \begin{example}[Boundness]
        Consider the following term: $$\lambda x.xy$$ In this example, $x$ is \tit{bound}, and $y$ is \tit{free}.
    \end{example}

    \begin{frameddefn}{Notable combinators}
        The following are some of the \tbf{notable combinators}:
        \begin{equation*}
            \begin{split}
                \mathrm I &\equiv \lambda x.x \\
                \mathrm K &\equiv \lambda xy.x \\
                \mathrm O &\equiv \lambda xy.y \\
                \mathrm S &\equiv \lambda xyz.xz(yz) \\
                \mathrm B &\equiv \lambda fgx.f(gx) \\
                \mathrm C &\equiv \lambda abc.acb \\
                \mathrm W &\equiv \lambda xy.xyy
            \end{split}
        \end{equation*}
    \end{frameddefn}

    \begin{frameddefn}{Free variables}
        Given a $\lambda$-term, the function $$\func{\mathrm{free}}{\Lambda}{\mathcal P(\mathrm{Var})}$$ returns the \tbf{set of free variables in $M$}, and it is defined recursively as follows:
        $$\soe{l}{
            \mathrm{free}(x) := \{x\} \\
            \mathrm{free}(MN) := \mathrm{free}(M) \cup \mathrm{free}(N) \\
            \mathrm{free}(\lambda x.M) := \mathrm{free}(M) - \{x\}
        }$$
    \end{frameddefn}

    \begin{frameddefn}{Substitution}
        The \tbf{substitution} operation is recursively defined by the following rules:
        \begin{gather*}
            x[N/x] = N \\ \\
            y[N/x] = y \\ \\
            (P Q)[N/x] = P[N/x] \ Q[N/x] \\ \\
            (\lambda y.P)[N/x] = \lambda y.(P[N/x]) \ \mathrm{if \ } y \neq x \\ \\
            (\lambda x.P)[N/x] = \lambda x.P
        \end{gather*}

        where $M[N/x]$ means that \tit{each instance of $x$ in $M$ is replaced with $N$}. Note that \tbf{only \tbf{free variables} may be substituted}.
    \end{frameddefn}

    \begin{framedlem}{Substitution lemma}
        Let $M, N, L \in \Lambda$; if $x \neq y$ and $x \notin \mathrm{free}(L)$, then $$M[M/x][L/y] \equiv M[L/y][N[L/y]/x]$$
    \end{framedlem}

    \begin{proof}
        By induction on the structure of $M$, the details are omitted.
    \end{proof}

    \begin{frameddefn}[label={inf rules}]{Inference rules}
        The following are the \tbf{inference rules} for the lambda caluclus:
        \begin{gather*}
            \rbk{\alpha} \ \lambda x.M \equiv (\lambda y.M)[y/x] \\ \\
            \rbk{\beta} \ (\lambda x.M)N \betaconv M[N/x] \\ \\
            \rbk{\mu} \ \dfrac{M \betaconv M'}{NM \betaconv NM'} \\ \\
            \rbk{\nu} \ \dfrac{M \betaconv M'}{MN \betaconv M'N} \\ \\
            \rbk{\xi} \ \dfrac{M \betaconv M'}{\lambda x.M \betaconv \lambda x.M'}
        \end{gather*}

        Note that the $\beta$-rule is effectively \tit{one step of the computation} described by the $\lambda$-term.
    \end{frameddefn}

    If $M \equiv N$ is provable in the $\lambda$-caluclus, it will be written as $$\lambda \vdash M \equiv N$$

    Additionally, if a term $N$ can be derived from $M$ through $\beta$-reductions $$M \betaconv \ldots \betaconv N$$ it will be written as $M \leadsto N$.

    \begin{frameddefn}{Normal form}
         If a term can be $\beta$-reduced, it is called \tbf{$\beta$-redex}, or simply \tbf{redex} (\tit{reducible expression}), and the reduced term is called \tbf{$\beta$-reduct}, or simply \tbf{reduct}.

         If a term has no redexes, it is said to be in \tbf{normal form}.
    \end{frameddefn}

    \begin{framedobs}{Variable capture}
        Consider the following $\lambda$-term: $$(\lambda xt.tx)(\lambda t.y) \betaconv \lambda t.t(\lambda t.y)$$ Note that the two $t$s are \tit{different}. In fact, underlining the $\lambda$-abstractions to which they are bounded to can help clarifying their distinction: $$(\lambda x\underline{t}.\underline{t}x)(\lambda t.y) \betaconv \lambda \underline{t}.\underline{t}(\lambda t.y)$$ Now, consider the following $\lambda$-term, similar to the previous one: $$(\lambda xy.yx)(\lambda t.y) \betaconv \lambda y.y(\lambda t.y)$$ This $\beta$-reduction created a problem, because now the two $y$s \tit{are the same}, even though they were not originally. In fact, the previous term can be relabeled as follows: $$(\lambda x \underline y. \underline y x)(\lambda t.y) \betaconv \lambda \underline y. \underline y (\lambda t. \underline y)$$ This happened because $$\mathrm{free}(\lambda t.y) = \{y\} - \{t\} = \{y\}$$ therefore $y$ was \tbf{captured} by the $y$ that was already present in the leftmost $\lambda$-abstraction. This phenomena is called \tbf{variable capturing}, and constitutes a problem when reducing $\beta$-redexes. In particular, to reduce this second $\lambda$-abstraction, it is necessary to apply a substitution, by using the $\alpha$ rule (refer to \cref{inf rules}): $$\lambda xy.yx = \lambda x(\lambda y.yx) = \lambda x.((\lambda y.yx)[u/y]) = \lambda x.(\lambda u.ux) = \lambda x u. ux$$ which means that the $\beta$-reduction can now be performed without any issue: $$(\lambda xu.ux)(\lambda t.y) \betaconv \lambda u.u(\lambda t.y)$$ where $y$ is still free. Note that it would not have been \tit{safe} to rename the other (free) $y$, because in general \tit{renaming free variables can create capturing problems as well}. For example, $y$ could have not been substituted with $t$, as it would otherwise be captured by the $t$ in the $\lambda$-term $\lambda t.y$, as follows: $$(\lambda t.y)[t/y] =\lambda t.t$$
    \end{framedobs}

    Fortunately, variable capturing can be solved by employing the following \tit{variable naming convention}.

    \begin{frameddefn}{Variable naming convention}
        To avoid variable capturing problems, it is sufficient to follow this \tbf{variable naming convention}: \tit{bound and free variables must have \underline{different} names between them}.

        From now on, it will be assumed that any $\beta$-reduction is performed by renaming opportunely the \tbf{bound} variables, such that in each step of the computation the naming convention is followed.
    \end{frameddefn}

    \begin{frameddefn}{Tuples}
        A \tbf{tuple} of the form $$(M_1, \ldots, M_k)$$ can be represented in $\lambda$-calculus as follows: $$\lambda x.xM_1 \ldots M_k$$ In $\lambda$-caluclus, tuples will be represented as follows $$[M_1, \ldots, M_k]$$
    \end{frameddefn}

    To access the elements of a tuple, \tit{projectors} are used, which are defined below.

    \begin{frameddefn}{Projector}
        A \tbf{projector} has the following form $$\lambda x.x\pi_j^k$$ where $$\pi_j^k \equiv \lambda x_1 \ldots x_k.x_j$$
    \end{frameddefn}

    \begin{example}[Projectors]
        Given a tuple $\lambda x.x M_1 \ldots M_k$, its $j$-th element can be accessed as follows: $$\lambda x.x \pi_j^k (\lambda x.x M_1 \ldots M_k) \betaconv (\lambda x.x M_1 \ldots M_k) \pi_j^k \betaconv \pi_j^k M_1 \ldots M_k \betaconv M_j$$
    \end{example}

    \begin{frameddefn}{Booleans}
        \tbf{Booleans} can be defined in $\lambda$-calculus as follows: $$\centeredsoe{\mathrm T \equiv \lambda xy.x \\ \mathrm F \equiv \lambda xy.y}$$
    \end{frameddefn}

    \begin{frameddefn}{Conditionals}
        \tbf{Conditionals} can be defined in $\lambda$-calculus as follows: $$\mathrm{ite} = \lambda xyz.xyz$$
    \end{frameddefn}

    Note that $\mathrm{ite}$ stands for \tit{\curlyquotes{if-then-else}}, and in fact, the term behaves exactly like a condition when used in conjunction with the $\lambda$-booleans.

    \begin{framedobs}{Conditionals}
        The term $\mathrm{ite}$ correctly behaves as a \tit{conditional} when used with $\mathrm T$ and $\mathrm F$. In fact, when used in an term such as $$\mathrm{ite} \ \mathrm C \ \mathrm A \ \mathrm B$$ if $\mathrm C$ is a $\lambda$-boolean, the term with be $\beta$-reduced to $\mathrm A$ if $\mathrm C \equiv \mathrm T$, and it will be evaluated to $\mathrm B$ if $\mathrm C \equiv \mathrm F$. Indeed
        \begin{equation*}
            \begin{split}
                \mathrm{ite} \  \mathrm T \ \mathrm A \ \mathrm B &\equiv (\lambda xyz.xyz) \ \mathrm T \ \mathrm A \ \mathrm B \\
                                                                  &\betaconv \mathrm T \ \mathrm A \ \mathrm B \\
                                                                  &\equiv (\lambda xy.x) \ \mathrm A \ \mathrm B \\
                                                                  &\betaconv \mathrm A \\
            \end{split}
        \end{equation*}
        and
        \begin{equation*}
            \begin{split}
                \mathrm{ite} \  \mathrm F \ \mathrm A \ \mathrm B &\equiv (\lambda xyz.xyz) \ \mathrm F \ \mathrm A \ \mathrm B \\
                                                                  &\betaconv \mathrm F \ \mathrm A \ \mathrm B \\
                                                                  &\equiv (\lambda xy.y) \ \mathrm A \ \mathrm B \\
                                                                  &\betaconv \mathrm B \\
            \end{split}
        \end{equation*}
    \end{framedobs}

    \begin{frameddefn}{Church numerals}
        The \tbf{Church numerals} are defined by a mapping between natural numbers $\N$ and $\lambda$-abstractions: $$\funcmap{\varphi}{\N}{\Lambda}{n}{\lambda xy. \underbrace{x ( \ldots ( x}_{n \ \mathrm{times}}y))}$$
    \end{frameddefn}

    The Church numeral corresponding to $n \in \N$ will be represented as $\underline n \in \Lambda$.

    \begin{example}[Church numerals]
        For example, the number $\underline 0$ is represented as $$\varphi(0) = \underline 0 = \lambda xy. y \equiv \mathrm F \equiv \mathrm O$$ number $\underline 1$ as $$\varphi(1) = \underline 1 = \lambda xy.xy \equiv \mathrm T \equiv \mathrm K$$ and number $\underline 2$ as $$\varphi(2) = \underline 2 = \lambda xy.x(xy)$$ and so on.
    \end{example}

    \begin{framedobs}[label={church_iterators}]{Church numerals are iterators}
        Note that Church numerals are \tbf{iterators}, in the sense that $\underline n$ replicates any input function $f$ for $n$ times
        \begin{equation*}
            \begin{split}
                \underline n \ f \ \chi &\equiv (\lambda xy.\underbrace{x(\ldots(x}_{n \ \mathrm{times}}y))) \ f \ \chi \\
                                        &\betaconv \underbrace{f(\ldots(f}_{n \ \mathrm{times}}\chi))
            \end{split}
        \end{equation*}
    \end{framedobs}

    The following are some important $\lambda$-functions for Church numerals:
    
    \begin{itemize}
        \item \tbf{successor function}: $$\underline s \equiv \lambda xyz. xy(yz)$$ which given a Church numeral $\underline n$, it returns $\underline {n + 1}$, which is easy to show
            \begin{equation*}
                \begin{split}
                    \underline s \ \underline n &\equiv (\lambda abc.ab(bc)) \ (\lambda xy. \underbrace{x ( \ldots ( x}_{n \ \mathrm{times}}y))) \\
                                              &\betaconv \lambda bc.(\lambda xy. \underbrace{x ( \ldots ( x}_{n \ \mathrm{times}}y)) \ b \ (bc)) \\
                                              &\betaconv \lambda bc. (\lambda y. \underbrace{b ( \ldots ( b}_{n \ \mathrm{times}}y)) \ (bc)) \\
                                              &\betaconv \lambda bc. \underbrace{b ( \ldots ( b}_{n +1 \ \mathrm{times}} c)) \\
                \end{split}
            \end{equation*}
        \item \tbf{is-zero function}: $$\underline z \equiv \lambda f.f(\lambda t.\mathrm F) \mathrm T$$ which given a Church numeral $\underline n$, it returns $\mathrm T$ if and only if $\underline n$ is $\underline 0$, and it can be proven at follows
            \begin{equation*}
                \begin{split}
                    \underline z \ \underline 0 &\equiv (\lambda f.f(\lambda t.\mathrm F) \mathrm T) \ (\lambda xy.y) \\
                                                &\betaconv (\lambda xy.y) (\lambda t.\mathrm F) \mathrm T \\
                                                &\betaconv \mathrm T
                \end{split}
            \end{equation*}
            and
            \begin{equation*}
                \begin{split}
                    \underline z \ \underline n &\equiv (\lambda f.f(\lambda t.\mathrm F) \mathrm T) \ (\lambda xy. \underbrace{x ( \ldots ( x}_{n \ \mathrm{times}}y))) \\
                                                &\betaconv (\lambda xy. \underbrace{x ( \ldots ( x}_{n \ \mathrm{times}}y))) (\lambda t.\mathrm F)\mathrm T \\
                                                &\betaconv \underbrace{(\lambda t.\mathrm F)(\ldots ((\lambda t.\mathrm F)}_{n \ \mathrm{times}} \mathrm T)) \\
                                                &\betaconv \mathrm F
                \end{split}
            \end{equation*}
        \item \tbf{addition function}: $$\mathrm{add} \equiv \lambda ab.a \ \underline s \ b$$ a proof of this function is omitted, but it can be intuitively explained by using \cref{church_iterators}, which suggests that if $\underline a$ and $\underline b$ are two Church numerals, then $\underline a \ \underline s \ \underline b$ is the repeated application of $\underline s$ exactly $a$ times to $\underline b$
    \end{itemize}

    \begin{frameddefn}{Fixed point}
        Given a function $\func{f}{X}{Y}$, an element $x \in X$ is said to be a \tbf{fixed point of $f$} if and only if $f(x) = x$.
    \end{frameddefn}

    \begin{example}[Fixed points]
        Given a function $f(x) = x^2 - 3x + 4$, $x = 2$ is a \tit{fixed point} of $f$, because $$f(x) = 2^2 - 3 \cdot 2 + 4 = 4 - 6 + 4 = 2 = x$$ and thus $f(x) = x$.
    \end{example}

    \begin{example}[Functions are fixed points]
        Consider the following function $$F(g) :\equiv h(x) = \soe{ll}{1 & x = 0 \\ x \cdot g(x - 1) & x > 0}$$ that takes a function as input, an returns a function $h$; for instance, plugging in the following function $$\func{\succfn}{x}{x + 1}$$ we get that $F$ returns the following function $$F(\succfn) \equiv h(x) = \soe{ll}{1 & x = 0 \\ x \cdot \succfn(x - 1) = x \cdot x = x^2 & x > 0}$$ which is the function that returns $1$ if $x = 0$, and $x^2$ otherwise.

        It's easy to check that the \tit{fixed point} of $F$ is the following function: $$\mathrm{fact}(x) := \soe{ll}{1 & x = 0 \\ x \cdot \mathrm{fact}(x - 1) & x > 0}$$ which computes the factorial of $x$, because $$F(\mathrm{fact}) \equiv h(x) = \soe{ll}{1 & x = 0 \\ x \cdot \mathrm{fact}(x -1) & x > 0 } \equiv \mathrm{fact}$$
    \end{example}

    \begin{frameddefn}{Kleene's combinator}
        The \tbf{fixed point operator}, \tbf{$\mathrm Y$ combinator} or \tbf{Kleene's combinator} (named after \href{https://it.wikipedia.org/wiki/Stephen_Kleene}{Stephen Kleene}) is defined as follows: $$\mathrm Y \equiv \lambda f.(\lambda x.f(xx))(\lambda x.f(xx))$$ The $\mathrm Y$ combinator can be alternatively defined as follows: $$\mathrm Y \equiv (\lambda xy. y(xxy))(\lambda xy.y(xxy))$$
    \end{frameddefn}

    \begin{framedprop}{Fixed point operator}
        Given a function, Kleene's combinator returns its fixed point.
    \end{framedprop}

    \begin{proof}
        If the Kleene's combinator can return the fixed point of a given function $h$, it means that $\mathrm Y h$ is $h$'s fixed point. Therefore, the statement that has to be proved is that $$h (\mathrm Y h) \equiv \mathrm Y h$$ This can be proved for both formulations of the $\mathrm Y$ combinator, as follows:
        \begin{equation*}
            \begin{split}
                \mathrm Y h &\betaconv (\lambda f .(\lambda x.f(xx)) (\lambda x.f(xx))) h \\
                            &\betaconv (\lambda x.h(xx))(\lambda x.h(xx)) \\
                            &\betaconv h(\lambda x.h(xx) \lambda x. h(xx)) \\
                            &\betaconv h (\mathrm Y h)
            \end{split}
        \end{equation*}
        and for the alternative formulation
        \begin{equation*}
            \begin{split}
                \mathrm Y h &\betaconv ((\lambda xy.y(xxy))(\lambda xy'.y'(xxy')))h \\
                            &\betaconv (\lambda y.y ((\lambda xy'.y'(xxy'))(\lambda xy''.y''(xxy''))y))h \\
                            &\betaconv h ((\lambda xy'.y'(xxy'))(\lambda xy''.y''(xxy''))h) \\
                            &\betaconv h(\mathrm Y h)
            \end{split}
        \end{equation*}
    \end{proof}

    Note that the $\mathrm Y$ combinator can be used to perform \tit{recursion} inside $\lambda$-calculus, because of the following property: $$\centeredsoe{h(\mathrm Y h) = \mathrm Y h \\ h(h(\mathrm Y h)) = h(\mathrm Y h) = \mathrm Y h \\ \vdots \\ h (\ldots (h(\mathrm Y h))) = \mathrm Y h}$$ \todo{go deeper into rec}

    % \begin{framedcor}{}
    %
    % \end{framedcor}

    \begin{frameddefn}{Numeric function}
        A \tbf{numeric function} is a map $\func{f}{\N^p}{\N}$ for some $p$.
    \end{frameddefn}

    \begin{frameddefn}{$\lambda$-definable function}
        A function is said to be \tbf{$\lambda$-definable} if there exists a closed term $F$ such that $$F \ \underline{n_1} \ldots \underline{n_p} \equiv \underline{f(n_1, \ldots, n_p)}$$ If that is the case, $f$ is said to be \tbf{$\lambda$-defined} by $F$.
    \end{frameddefn}

    \begin{frameddefn}{Initial functions}
        The following are the so called \tbf{initial functions}:
        \begin{equation*}
            \begin{split}
                U_r^i(x_1, \ldots, x_r) &= x_i, \quad 1 \le i \le r \\
                \succfn(n) &= n + 1 \\
                \mathrm{zero}(n) &= 0
            \end{split}
        \end{equation*}
    \end{frameddefn}

    In particular, the first equations are called \tbf{projection functions}, the second is the \tbf{successor function} and the last is called \tbf{constant function}.

    \begin{frameddefn}{Composition}
        Given an $m$-ary function $h(x_1, \ldots, x_m)$ and $k$ $m$-ary functions $$g_1(x_1, \ldots, x_k), \ldots, g_m(x_1, \ldots, x_k)$$, the \tbf{composition operator} is defined as follows: $$f := h \circ (g_1, \ldots, g_m)$$ where $$f(\vec x) = h(g_1(\vec x), \ldots, g_m(\vec x))$$
    \end{frameddefn}

    \begin{framedlem}[label={lem:composition}]{Composition}
        The $\lambda$-definable functions are closed under \tit{composition}.
    \end{framedlem}

    \begin{proof}
        Let $h, g_1, \ldots, g_m$ be defined by the $\lambda$-terms $H, G_1, \ldots, G_m$ respectively. Then $$F \equiv \lambda \vec x. H(G_1\vec x) \ldots (G_m \vec x)$$ $\lambda$-defines $h \circ (g_1, \ldots, g_m)$.
    \end{proof}
    
    \begin{frameddefn}{Primitive recursion}
        Given a $k$-ary function $g(x_1, \ldots, x_k)$ and a $(k + 2)$-ary function $h(y, z, x_1, \ldots, x_k)$, the \tbf{primitive recursion operator} is a $(k + 1)$-ary function $\rho$ is defined as follows: $$f := \rho (g, h)$$ where
        \begin{equation*}
            \begin{split}
                f(0, \vec n) &= g(\vec x) \\
                f(\succfn(n), \vec x) &= h(y, f(y, \vec x), \vec x)
            \end{split}
        \end{equation*}
    \end{frameddefn}
    
    \begin{framedlem}[label={lem:prim_rec}]{Primitive recursion}
        The $\lambda$-definable functions are closed under \tit{primitive recursion}.
    \end{framedlem}

    \begin{proof}
        Let $f$ be a function such that
        \begin{equation*}
            \begin{split}
                f(0) &= g \\
                f(k + 1) &= h(f(k), k)
            \end{split}
        \end{equation*}
        which is a weaker version of the \tit{primitive recursion operator}, but the proof for general $\vec n$ is similar (details are omitted).

        Consider the following term $$T \equiv \lambda p. [\underline s (p \ \mathrm T), H(p \ \mathrm F)  (p \ \mathrm T)]$$ where $H$ $\lambda$-defines $h$, and note how it computes over the following input
        \begin{equation*}
            \begin{split}
                T \ [\underline k, \underline{f(x)}] &\leadsto [\underline s \ \underline k, H \ \underline{f(x)} \ \underline k] \\
                                                     &\betaconv [\underline{k +1}, H \ \underline{f(x)} \ \underline k] \\
                                                     &\equiv [\underline{k + 1}, \underline{f(k +1)}]
            \end{split}
        \end{equation*}
        (the last step follows from $f$'s definition) therefore, $T$ describes the following function $$\funcmap{t}{\N \times \N}{\N \times \N}{(k, f(k))}{(k + 1, f(k + 1))}$$ Hence, by induction, it follows that $$[\underline k, \underline{f(k + 1)}] \equiv T^k \ [\underline 0, \underline{f(0)}]$$ and because Church numerals are iterators (as shown in \cref{church_iterators}), it follows that $$\underline{f(k)} \equiv \underline k \ T \ [\underline 0, \underline{f(0)}] \ \mathrm F$$ Finally, $f$ can be $\lambda$-defined by the following term $$F \equiv \lambda k. k \ T \ [\underline 0, \underline G] \ \mathrm F$$ where $G$ $\lambda$-defines $g$.
    \end{proof}

    \begin{frameddefn}{Minimalization}
        Given a $(k + 1)$-ary function $f(y, x_1, \ldots, x_k)$, the \tbf{minimization operator} is a $k$-ary function $\mu(f)$ defined as follows: $$\mu(f)(\vec x) = z \iff \forall i \in [0, z - 1] \quad f(i, \vec x) > 0 \land f(z, \vec x) = 0$$
    \end{frameddefn}

    In other words, that \tit{minimization operator} seeks the smallest argument that causes the function to return zero. If there is no such argument, or if an argument is encountered for which $f$ is not defined, then the search never terminates, and $\mu(f)$ is not defined for $\vec x$.

    \begin{framedlem}[label={lem:minimalization}]{Minimalization}
        The $\lambda$-definable functions are closed under minimalization
    \end{framedlem}

    \begin{proof}
        TODO
    \end{proof}

    \begin{frameddefn}{Recursive functions}
        The class $\mathcal R$ is the smallest class of \tit{numeric functions} that contains \tit{all initial functions}, and is closed under \tit{composition}, \tit{primitive recursion} and \tit{minimization}.
    \end{frameddefn}

    \begin{framedthm}{Recursive functions}
        All recursive functions are $\lambda$-definable, therefore $$\mathcal R \subsetneq \Lambda$$
    \end{framedthm}

    \begin{proof}
        The following terms $\lambda$-define \tit{initial functions}:
        \begin{equation*}
            \begin{split}
                U_p^i &\equiv \pi_j^k \\
                \succfn &\equiv \underline s \\
                \mathrm{zero} &\equiv \underline 0
            \end{split}
        \end{equation*}

        Thus, the theorem follows directly from \cref{lem:composition}, \cref{lem:prim_rec} and \cref{lem:minimalization}.
    \end{proof}

    \section{Exercises}

    \begin{framedprob}{Solve for $X$}
        Find $X$ such that $$Xx = \lambda t.t (Xx)$$
    \end{framedprob}
    
    \solution{
        The term is $$X \equiv (\lambda fbt. t(fb)) X$$ because
        \begin{equation*}
            \begin{split}
                Xx &\betaconv (\lambda fbt .t(fb)) X x \\
                   &\betaconv (\lambda bt. t(Xb)) x \\
                   &\betaconv \lambda t.t(Xx)
            \end{split}
        \end{equation*}
    }
    
    \begin{framedprob}{Solve for $H$}
        Find $H$ such that $$H (\lambda x_1 x_2 x_3.P) = \lambda x_3 x_2 x_1 . P$$
    \end{framedprob}

    \solution{
        The term is $$H \equiv \lambda f x_3 x_2 x_1 . f x_1 x_2 x_3$$ because
        \begin{equation*}
            \begin{split}
                H (\lambda x_1 x_2 x_3.P) &\betaconv (\lambda f x_3 x_2 x_1 . f x_1 x_2 x_3) (\lambda x_1 x_2 x_3.P) \\
                                                  &\betaconv \lambda x_3 x_2 x_1 . (\lambda x_1 x_2 x_3.P) x_1 x_2 x_2 \\
                                                  &\betaconv \lambda x_3 x_2 x_1 .P
            \end{split}
        \end{equation*}
    }

    \begin{framedprob}{Solve for $X$}
        Find $X$ such that $$X xyz = Xz(uv)$$
    \end{framedprob}

    \solution{
        The term is $$X \equiv (\lambda tabc.tc(uv)) X$$ because
        \begin{equation*}
            \begin{split}
                (\lambda tabc.tc(uv)) X xyz &\betaconv (\lambda abc.Xc(uv)) xyz \\
                                            &\betaconv Xz(uv)
            \end{split}
        \end{equation*}
    }

    \begin{framedprob}{Solve for $\Delta$}
        Find $\Delta$ such that $$\soe{l}{\Delta \mathrm S = y_1 \\ \Delta \mathrm K = y_2 \\ \Delta \mathrm I = y_3}$$
    \end{framedprob}

    \solution{
        Assume that $$\Delta \equiv \lambda x.x P_1 P_2 P_3$$ for some $\lambda$-terms $P_1$, $P_2$ and $P_3$; then $$\soe{l}{\Delta \mathrm S \betaconv \mathrm S \ P_1 P_2 P_3 \betaconv P_1 P_3(P_2 P_3)\\ \Delta \mathrm K \betaconv \mathrm K \ P_1 P_2 P_3 \betaconv P_1 P_3\\ \Delta \mathrm I \betaconv \mathrm I \ P_1 P_2 P_3 \betaconv P_1 P_2 P_3}$$ However, this cannot be a correct assumption, because if $$\Delta \mathrm K \betaconv P_1P_3 = y_2$$ then $$\Delta \mathrm S \betaconv P_1P_3 (P_2P_3) = y_2(P_2P_3) \neq y_1$$ which means that $\Delta \mathrm S$ cannot be evaulated to $y_1$. This issue can be solved by assuming that $$\Delta \equiv \lambda x.x P_1 P_2 P_3 P_4$$ for some other term $\lambda$-term $P_4$, in fact $$\soe{l}{\Delta \mathrm S \betaconv \mathrm S \ P_1 P_2 P_3 P_4 \betaconv P_1 P_3(P_2 P_3)P_4\\ \Delta \mathrm K \betaconv \mathrm K \ P_1 P_2 P_3 P_4 \betaconv P_1 P_3 P_4 \\ \Delta \mathrm I \betaconv \mathrm I \ P_1 P_2 P_3 P_4 \betaconv P_1 P_2 P_3 P_4}$$ and if $P_1 = \lambda xy.y$ then $$\Delta \mathrm K \betaconv P_1 P_3 P_4 \betaconv (\lambda xy.y) P_3 P_4 \betaconv P_4$$ whch means that $P_4$ must be $y_2$. Moreover $$\Delta \mathrm I \betaconv P_1P_2P_3P_4 \equiv (\lambda xy.y)P_2P_3 y_2 \betaconv P_3y_2 = y_3 \iff P_3 = \lambda t.y_3$$ and finally $$\Delta \mathrm S\betaconv P_1P_3(P_2P_3)P_4 \equiv (\lambda xy.y)(\lambda t.y_3)(P_2(\lambda t.y_3))y_2 \iff P_2 = \lambda ab.y_1$$
    }

\end{document}
